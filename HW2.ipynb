{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DS-GA 1022 Homework 2\n",
    "Code throughout adapted from DS-GA 1011 2018 Lab Sessions and previously submitted HW1\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "'''\n",
    "First we'll load our pre-trained embedding.\n",
    "We'll prefix our embeddings with a PAD and OOV\n",
    "'''\n",
    "\n",
    "#switch between linux laptop and windows desktop\n",
    "embedding_home = '/home/gandalf/NYUCDS/DS-GA1011/lab5/'\n",
    "if not os.path.isdir(embedding_home):\n",
    "    embedding_home = 'C:\\\\development\\\\NYUCDS\\\\DSGA1011\\\\'\n",
    "\n",
    "#pre-trained embeddings from fasttext\n",
    "embedding_source = 'wiki-news-300d-1M.vec' \n",
    "words_to_load = 50000 #limit our vocab to something reasonable\n",
    "embedding_dim = 300\n",
    "num_embeddings = words_to_load +2 #leave room for our pad and OOV\n",
    "\n",
    "words = {}\n",
    "idx2words = {}\n",
    "ordered_words = []\n",
    "loaded_embeddings = np.zeros((words_to_load+2, embedding_dim))\n",
    "\n",
    "#prefix with PAD and OOV\n",
    "PAD_IDX = 0\n",
    "OOV_IDX = 1\n",
    "words['<PAD>'] = PAD_IDX\n",
    "idx2words[PAD_IDX] = '<PAD>'\n",
    "words['<OOV>'] = OOV_IDX\n",
    "idx2words[OOV_IDX] = '<OOV>'\n",
    "loaded_embeddings[OOV_IDX,:] = np.random.normal(size=embedding_dim)\n",
    "\n",
    "with open(embedding_home + embedding_source, \"r\", encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[i+2, :] = np.asarray(s[1:])\n",
    "        words[s[0]] = i+2\n",
    "        idx2words[i+2] = s[0]\n",
    "        ordered_words.append(s[0])\n",
    "\n",
    "loaded_embeddings = torch.tensor(loaded_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's load our data now\n",
    "'''\n",
    "\n",
    "#switch between linux laptop and windows desktop\n",
    "data_src = '/home/gandalf/Dropbox/NYU CDS/DS-GA 1011 NLP/HW2/hw2_data/'\n",
    "if not os.path.isdir(data_src):\n",
    "    data_src = 'E:\\\\cloudstation\\\\Dropbox\\\\NYU CDS\\\\DS-GA 1011 NLP\\\\HW2\\\\hw2_data\\\\'\n",
    "\n",
    "#our pre-processed SNLI files\n",
    "train_snli_name = 'snli_train.tsv' \n",
    "val_snli_name = 'snli_val.tsv' \n",
    "\n",
    "def index_it(sentence):\n",
    "    s_i = []\n",
    "    for w in sentence.split():\n",
    "        if w not in words:\n",
    "            s_i.append(OOV_IDX)\n",
    "        else:\n",
    "            s_i.append(words[w])\n",
    "    return s_i\n",
    "\n",
    "\n",
    "#load our data, it is in a tab delimited row, with permise, hypothesis, label\n",
    "#our sentences are already pre-procesed, we just need to split\n",
    "def loadsnli(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f,delimiter='\\t')\n",
    "        next(reader)#skip the header\n",
    "        for line in reader:\n",
    "            prem = index_it(line[0])\n",
    "            hypo = index_it(line[1])\n",
    "\n",
    "            if (line[2] == 'neutral'):\n",
    "                target = 0\n",
    "            elif (line[2] == 'entailment'):\n",
    "                target = 1\n",
    "            elif (line[2] == 'contradiction'):\n",
    "                target = 2\n",
    "            else:\n",
    "                target = 3 # shouldn't ever happen\n",
    "            data.append((prem,hypo,target))\n",
    "    return np.array(data)\n",
    "            \n",
    "train_data = loadsnli(data_src+train_snli_name)\n",
    "val_data = loadsnli(data_src+val_snli_name)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's inspect some things about our data before continuing\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#check the classes\n",
    "cls = {}\n",
    "p_lens = []\n",
    "h_lens = []\n",
    "cl = 0\n",
    "proposed_max_cutoff = 35\n",
    "for val in train_data:\n",
    "    v = val[2]\n",
    "    if not v in cls:\n",
    "        cls[v] = 1\n",
    "    else:\n",
    "        cls[v] = cls[v] + 1\n",
    "    p_lens.append(len(val[0]))\n",
    "    h_lens.append(len(val[1]))\n",
    "    if len(val[0]) < proposed_max_cutoff:\n",
    "        cl = cl +1\n",
    "\n",
    "p_lens = np.array(p_lens)\n",
    "h_lens = np.array(h_lens)\n",
    "\n",
    "print (\"our target class:\",cls)\n",
    "print(stats.describe(p_lens))\n",
    "print(stats.describe(h_lens))\n",
    "print(\"% fit in cutoff:\", cl/1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our dataset for our investigations\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prem_list, hypo_list, target_list, max_sentenance_length):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.prem_list = prem_list\n",
    "        self.hypo_list = hypo_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentenance_length = max_sentenance_length\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prem_list)#they should all be the same\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        prem_token_idx = self.prem_list[key][:self.max_sentenance_length]\n",
    "        prem_token_len = len(prem_token_idx)\n",
    "        \n",
    "        prem_token_idx = np.pad(np.array(prem_token_idx), \n",
    "                                pad_width=((0,self.max_sentenance_length-prem_token_len)), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        hypo_token_idx = self.hypo_list[key][:self.max_sentenance_length]\n",
    "        hypo_token_len = len(hypo_token_idx)\n",
    "        \n",
    "        hypo_token_idx = np.pad(np.array(hypo_token_idx), \n",
    "                                pad_width=((0,self.max_sentenance_length-hypo_token_len)), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        \n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, prem_token_len, hypo_token_idx, hypo_token_len,label]\n",
    "\n",
    "        \n",
    "\n",
    "BATCH_SIZE = 32 #chosen based on previous experience\n",
    "MAX_SENTENANCE_LENGTH = 35 #chosed from above\n",
    "\n",
    "train_dataset = SNLIDataset(train_data[:,0], train_data[:,1], train_data[:,2], MAX_SENTENANCE_LENGTH)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_data[:,0], val_data[:,1], val_data[:,2], MAX_SENTENANCE_LENGTH)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Our CNN model, with 2 1d convolutions, and a non-linear relu after each.\n",
    "Maxpooling is done across the seq len\n",
    "Both sentences are run through the same convolution model, and concatinated \n",
    "before going through another linear and non-linear transformation\n",
    "'''\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb, hidden_size, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.hidden_size =  hidden_size\n",
    "        \n",
    "        #use our previous loaded embedding matrix\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.embedding.load_state_dict({'weight': emb})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        \n",
    "        #self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2 ): \n",
    "        batch_size, seq_len = x1.size() #assumes seq_len is same for both, which our loader does\n",
    "\n",
    "        embed1 = self.embedding(x1.long())\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len, hidden1.size(-1))\n",
    "\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len, hidden1.size(-1))\n",
    "\n",
    "        #now maxppool, need to move things around to pool the right dimension\n",
    "        hidden1 = hidden1.transpose(1,2)\n",
    "        hidden1 = F.max_pool1d(hidden1, kernel_size=hidden1.size()[2])\n",
    "        hidden1 = hidden1.transpose(1,2)\n",
    "        hidden1 = hidden1.view(batch_size, self.hidden_size)\n",
    "                \n",
    "        embed2 = self.embedding(x2.long())\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "\n",
    "        #now maxppool, need to move things around to pool the right dimension\n",
    "        hidden2 = hidden2.transpose(1,2)\n",
    "        hidden2 = F.max_pool1d(hidden2, kernel_size=hidden2.size()[2])\n",
    "        hidden2 = hidden2.transpose(1,2)\n",
    "        hidden2 = hidden2.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        full = torch.cat((hidden1, hidden2), dim=1)\n",
    "        \n",
    "        #other ways were examined, but produced consistently worse results.\n",
    "        #full = torch.mul(hidden1, hidden2)\n",
    "        \n",
    "        full = self.linear1(full)\n",
    "        full = F.relu(full)\n",
    "        \n",
    "        logits = self.linear2(full)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For the RNN, a single-layer, bi-directional GRU will suffice.\n",
    "#We can take the last hidden state as the encoder output. \n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb, hidden_size, num_classes):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size =  hidden_size\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        #use our previous loaded embedding matrix\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.embedding.load_state_dict({'weight': emb})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "          \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, 1, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2 ): \n",
    "        batch_size, seq_len = x1.size() #assumes seq_len is same for both, which our loader does\n",
    "\n",
    "        h1 = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        \n",
    "        embed1 = self.embedding(x1.long())\n",
    "        \n",
    "        # pack padded sequence\n",
    "        #embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths1.numpy(), batch_first=True)\n",
    "        \n",
    "        #before working to sort, pack, unsort, attempt to train un-altered, which returned satisfactory results\n",
    "        #in a reasonable time.\n",
    "        \n",
    "        \n",
    "        #run the RNN\n",
    "        rnn_out, h1 = self.gru(embed1, h1)\n",
    "        \n",
    "        #sum accross the sequences\n",
    "        h1 = torch.sum(h1, dim=0)\n",
    "        \n",
    "        \n",
    "        h2 = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        \n",
    "        embed2 = self.embedding(x2.long())\n",
    "        \n",
    "        # pack padded sequence\n",
    "        #embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths2.numpy(), batch_first=True)\n",
    "        \n",
    "        # run the RNN\n",
    "        rnn_out, h2 = self.gru(embed2, h2)\n",
    "        \n",
    "        \n",
    "        #sum accross the sequences\n",
    "        h2 = torch.sum(h2, dim=0)\n",
    "        \n",
    "        #print(\"hidden1:\", hidden1.size())\n",
    "        #print(\"hidden2:\", hidden2.size())\n",
    "        \n",
    "        full = torch.cat((h1, h2), dim=1)\n",
    "        #full = torch.mul(hidden1, hidden2)\n",
    "        \n",
    "        full = self.linear1(full)\n",
    "        full = F.relu(full)\n",
    "        logits = self.linear2(full)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1, data2, lengths2, labels in loader:\n",
    "        data_batch1, lengths_batch1, data_batch2, lengths_batch2, label_batch = data1, lengths1, data2, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1, data_batch2, lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "#code below was used to experiment and confirm correct running.\n",
    "#Actual search of hyperparamters was done using SNLIModel.py \n",
    "#(an addapted script version of this notebook) on HPC/prince\n",
    "\n",
    "\n",
    "model = CNN(emb=loaded_embeddings, hidden_size=200, num_classes=3)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 4 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "print(\"training.....\")\n",
    "\n",
    "report_progress = False\n",
    "\n",
    "time_start = time.time()\n",
    "max_val = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, lengths1, data2, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1, data2, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 1000 == 0 and report_progress:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            if (val_acc > max_val):\n",
    "                max_val = val_acc\n",
    "            \n",
    "            if (val_acc + (0.1 * max_val) < max_val ):\n",
    "                break\n",
    "                \n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "    train_acc = test_model(train_loader, model)\n",
    "    val_acc = test_model(val_loader, model)\n",
    "    print('Epoch: [{}/{}], Train Acc, {} Validation Acc: {}'.format(\n",
    "                   epoch+1, num_epochs, train_acc, val_acc))\n",
    "\n",
    "    if (val_acc + (0.1 * max_val) < max_val ):\n",
    "        break;\n",
    "            \n",
    "time_end = time.time()\n",
    "print(\"Done in \", time_end- time_start)\n",
    "print(\"Best Val ACC seen\", max_val)\n",
    "print(\"Final Val ACC:\", test_model(val_loader, model))\n",
    "\n",
    "##Actual search of hyperparamters was done using SNLIModel.py (an addapted script version of this notebook) on HPC/prince\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our two best models, save from our exploration on HPC/prince\n",
    "\n",
    "with open('c:\\\\development\\\\NYUCDS\\\\DSGA1011\\\\hw2\\\\CNN2_models.pkl', 'rb') as f:\n",
    "    res_run = pickle.load(f)\n",
    "\n",
    "print(res_run.keys())\n",
    "    \n",
    "best_cnn_model = CNN(emb=loaded_embeddings, hidden_size=200, num_classes=3)\n",
    "best_cnn_model.state_dict = res_run['CNN-1']\n",
    "\n",
    "with open('c:\\\\development\\\\NYUCDS\\\\DSGA1011\\\\hw2\\\\RNN1_models.pkl', 'rb') as f:\n",
    "    res_run = pickle.load(f)\n",
    "\n",
    "print(res_run.keys())\n",
    "    \n",
    "best_rnn_model = RNN(emb=loaded_embeddings, hidden_size=200, num_classes=3)\n",
    "best_rnn_model.state_dict = res_run['RNN-200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the first 20 with predictions to examine some correct/incorrect cases\n",
    "\n",
    "def lookup_label(val):\n",
    "    if val == 0:\n",
    "        return 'neutral'\n",
    "    if val == 1:\n",
    "        return 'entailment'\n",
    "    if val == 2:\n",
    "        return 'contradiction'\n",
    "    return 'unknown'\n",
    "\n",
    "val_fixed_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=1)\n",
    "\n",
    "f = open(data_src+val_snli_name, 'r', encoding='utf-8' )\n",
    "f.readline()# skip header\n",
    "\n",
    "\n",
    "seen = 0\n",
    "best_cnn_model.eval()\n",
    "for data1, lengths1, data2, lengths2, labels in val_fixed_loader:\n",
    "    data_batch1, lengths_batch1, data_batch2, lengths_batch2, label_batch = data1, lengths1, data2, lengths2, labels\n",
    "    outputs = F.softmax(model(data_batch1, lengths_batch1, data_batch2, lengths_batch2), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    \n",
    "    print(f.readline().strip(),':','vs',lookup_label(predicted))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    seen += 1\n",
    "    if seen > 20:\n",
    "        break\n",
    "    \n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNLI data and run each genre through both \"best\" CNN and RNN models\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENANCE_LENGTH = 35\n",
    "        \n",
    "#load our data, it is in a tab delimited row, with permise, hypothesis, label, genre\n",
    "#our sentences are already pre-procesed, we just need to split\n",
    "def loadmnli(filename):\n",
    "    datas = {}\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,delimiter='\\t')\n",
    "        next(reader)#skip the header\n",
    "        for line in reader:\n",
    "            prem = index_it(line[0])\n",
    "            hypo = index_it(line[1])\n",
    "\n",
    "            if (line[2] == 'neutral'):\n",
    "                target = 0\n",
    "            elif (line[2] == 'entailment'):\n",
    "                target = 1\n",
    "            elif (line[2] == 'contradiction'):\n",
    "                target = 2\n",
    "            else:\n",
    "                target = 3 # shouldn't ever happen\n",
    "                \n",
    "            genre = line[3]\n",
    "            \n",
    "            if not genre in datas:\n",
    "                datas[genre] = []\n",
    "                \n",
    "            datas[genre].append((prem,hypo,target))\n",
    "            \n",
    "    return datas\n",
    "            \n",
    "val_mnli_name = 'mnli_val.tsv'\n",
    "val_mnli_data = loadmnli(data_src+val_mnli_name)\n",
    "\n",
    "#this assumes we've previous loaded our two best models\n",
    "\n",
    "print(\"genre\\tCNN Acc\\tRNN Acc\" )\n",
    "\n",
    "for genre in val_mnli_data.keys():\n",
    "    \n",
    "    vdata = np.array(val_mnli_data[genre])\n",
    "    \n",
    "    val_m_dataset = SNLIDataset(vdata[:,0], vdata[:,1], vdata[:,2], MAX_SENTENANCE_LENGTH)\n",
    "    val_m_loader = torch.utils.data.DataLoader(dataset=val_m_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    val_CNN_acc = test_model(val_m_loader, best_cnn_model)\n",
    "    val_RNN_acc = test_model(val_m_loader, best_rnn_model)\n",
    "    print(genre, val_CNN_acc, val_RNN_acc, sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting method reused from HW1\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "#Display a chart of the data, given the label, a list of runs, and the number of \n",
    "#columns to display the legend.\n",
    "#assumes data is in {paramaters1:[[E1-trainacc,E1-valacc],[E2-trainacc,E2-valacc]...]}\n",
    "def show_chart(plot_label, res_run, cols):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for key in res_run:\n",
    "        arun = np.array(res_run[key])\n",
    "        y = arun[:,0] # unpack a list of pairs into two tuples\n",
    "        x = range(1,len(y)+1)\n",
    "        ax.plot(x, y, label = key + ' train', linestyle=\"--\")\n",
    "        y = arun[:,1] # unpack a list of pairs into two tuples\n",
    "        x = range(1,len(y)+1)\n",
    "        ax.plot(x, y, label = key + ' validate', linestyle=\"-\")\n",
    "\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.grid('on')\n",
    "    plt.title(plot_label)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Percent Accuracy\")\n",
    "    plt.legend()    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    lgd = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=cols)\n",
    "    plt.savefig(plot_label, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our data after each test, useful if we need to redo graphs or tables, etc.\n",
    "import pickle\n",
    "\n",
    "def save_data(run_name, res_run):\n",
    "\n",
    "    with open(run_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(res_run, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#load and build a new chart\n",
    "def graph_pickle(run_name, slots):\n",
    "    with open(run_name + '.pkl', 'rb') as f:\n",
    "        res_run = pickle.load(f)\n",
    "    show_chart(run_name, res_run,slots)\n",
    "    \n",
    "graph_pickle('/home/gandalf/NYUCDS/DS-GA1011/hw2/RNN2_results', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
